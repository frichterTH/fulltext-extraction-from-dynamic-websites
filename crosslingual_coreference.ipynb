{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb65fab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/florian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "import crosslingual_coreference\n",
    "from crosslingual_coreference import Predictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dc0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_fulltext_articles.csv')\n",
    "list_text = df['fulltext'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(map(str, list_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80642eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/florian/python_scripts/test/lib/python3.8/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arti \u001b[38;5;129;01min\u001b[39;00m list_text:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# avoids CPU overload\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m Predictor(\n\u001b[1;32m      6\u001b[0m         language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_sci_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m         )\n\u001b[0;32m---> 12\u001b[0m     list_articles_coref\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43marti\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolved_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/crosslingual_coreference/CrossLingualPredictor.py:107\u001b[0m, in \u001b[0;36mCrossLingualPredictor.predict\u001b[0;34m(self, text, advanced_resolve)\u001b[0m\n\u001b[1;32m    104\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [text]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# make predictions for individual chunks\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_json({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk}) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# determine doc_lengths to resolve overlapping chunks\u001b[39;00m\n\u001b[1;32m    110\u001b[0m doc_lengths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(doc_chunk\u001b[38;5;241m.\u001b[39msents)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m doc_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39m_spacy\u001b[38;5;241m.\u001b[39mpipe(chunks)\n\u001b[1;32m    112\u001b[0m ]\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/crosslingual_coreference/CrossLingualPredictor.py:107\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [text]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# make predictions for individual chunks\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# determine doc_lengths to resolve overlapping chunks\u001b[39;00m\n\u001b[1;32m    110\u001b[0m doc_lengths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(doc_chunk\u001b[38;5;241m.\u001b[39msents)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m doc_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39m_spacy\u001b[38;5;241m.\u001b[39mpipe(chunks)\n\u001b[1;32m    112\u001b[0m ]\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/predictors/predictor.py:55\u001b[0m, in \u001b[0;36mPredictor.predict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: JsonDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JsonDict:\n\u001b[1;32m     54\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_to_instance(inputs)\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/predictors/predictor.py:263\u001b[0m, in \u001b[0;36mPredictor.predict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: Instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JsonDict:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_reader\u001b[38;5;241m.\u001b[39mapply_token_indexers(instance)\n\u001b[0;32m--> 263\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sanitize(outputs)\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/models/model.py:191\u001b[0m, in \u001b[0;36mModel.forward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_on_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: Instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    Takes an [`Instance`](../data/instance.md), which typically has raw text in it, converts\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    that text into arrays using this model's [`Vocabulary`](../data/vocabulary.md), passes those\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    `torch.Tensors` into numpy arrays and remove the batch dimension.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/models/model.py:217\u001b[0m, in \u001b[0;36mModel.forward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    215\u001b[0m dataset\u001b[38;5;241m.\u001b[39mindex_instances(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m    216\u001b[0m model_input \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mmove_to_device(dataset\u001b[38;5;241m.\u001b[39mas_tensor_dict(), cuda_device)\n\u001b[0;32m--> 217\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_output_human_readable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    219\u001b[0m instance_separated_output: List[Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    220\u001b[0m     {} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39minstances\n\u001b[1;32m    221\u001b[0m ]\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp_models/coref/models/coref.py:180\u001b[0m, in \u001b[0;36mCoreferenceResolver.forward\u001b[0;34m(self, text, spans, span_labels, metadata)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m# Parameters\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    A scalar loss to be optimised.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Shape: (batch_size, document_length, embedding_size)\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexical_dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_field_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    182\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m spans\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    183\u001b[0m document_length \u001b[38;5;241m=\u001b[39m text_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py:102\u001b[0m, in \u001b[0;36mBasicTextFieldEmbedder.forward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     token_vectors \u001b[38;5;241m=\u001b[39m embedder(\u001b[38;5;28mlist\u001b[39m(tensors\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params_values)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# If there are multiple tensor arguments, we have to require matching names from the\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# TokenIndexer.  I don't think there's an easy way around that.\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     token_vectors \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_vectors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# To handle some very rare use cases, we allow the return value of the embedder to\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# be None; we just skip it in that case.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     embedded_representations\u001b[38;5;241m.\u001b[39mappend(token_vectors)\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/modules/token_embedders/pretrained_transformer_mismatched_embedder.py:137\u001b[0m, in \u001b[0;36mPretrainedTransformerMismatchedEmbedder.forward\u001b[0;34m(self, token_ids, mask, offsets, wordpiece_mask, type_ids, segment_concat_mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m# Parameters\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Shape: [batch_size, num_orig_tokens, embedding_size].\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Shape: [batch_size, num_wordpieces, embedding_size].\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matched_embedder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordpiece_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_concat_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_concat_mask\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# span_embeddings: (batch_size, num_orig_tokens, max_span_length, embedding_size)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# span_mask: (batch_size, num_orig_tokens, max_span_length)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m span_embeddings, span_mask \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mbatched_span_select(embeddings\u001b[38;5;241m.\u001b[39mcontiguous(), offsets)\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:242\u001b[0m, in \u001b[0;36mPretrainedTransformerEmbedder.forward\u001b[0;34m(self, token_ids, mask, type_ids, segment_concat_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m type_ids\n\u001b[0;32m--> 242\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scalar_mix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# The hidden states will also include the embedding layer, which we don't\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# include in the scalar mix. Hence the `[1:]` slicing.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    510\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    511\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 513\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2925\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    525\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 526\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 439\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    441\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_scripts/test/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_articles_coref = []\n",
    "\n",
    "for arti in list_text:\n",
    "# avoids CPU overload\n",
    "    predictor = Predictor(\n",
    "        language=\"en_core_sci_sm\",\n",
    "        device=-1,\n",
    "        model_name=\"spanbert\",\n",
    "        chunk_size=2500,\n",
    "        chunk_overlap=2,\n",
    "        )\n",
    "    list_articles_coref.append(predictor.predict(arti)[\"resolved_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53dcb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.5869483947754\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predictor = Predictor(\n",
    "        language=\"en_core_web_lg\",\n",
    "        device=-1,\n",
    "        model_name=\"info_xlm\",\n",
    "        chunk_size=2500,\n",
    "        chunk_overlap=2,\n",
    "        )\n",
    "\n",
    "article_coref = (predictor.predict(list_text[2])[\"resolved_text\"])\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb228176",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_core_sci_sm : 345.2646391391754\n",
    "en_core_sci_lg : 344.00879311561584\n",
    "en_core_sci_lg + info_xlm : 358.3959503173828\n",
    "en_core_sci_sm + info_xlm : 156.33155608177185\n",
    "en_core_web_sm + info_xml : 247.38766145706177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "631fe28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_sci_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de59e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(list_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd31a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(article_coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff004e",
   "metadata": {},
   "source": [
    "# SpaCy crosslingualcoreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8b2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_fulltext_articles.csv')\n",
    "list_text = df['fulltext'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896f500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<crosslingual_coreference.CrossLingualPredictorSpacy.CrossLingualPredictorSpacy at 0x7fbf8fbf9d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292ac52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/python_scripts/test/lib/python3.8/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(list_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5709de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[34, 35], [83, 84], [281, 282]], [[50, 50], [399, 400]], [[106, 112], [121, 121]], [[108, 110], [131, 133], [163, 165]], [[108, 112], [244, 248]], [[112, 112], [248, 248]], [[234, 235], [241, 248], [260, 260]], [[293, 296], [309, 332], [347, 352], [364, 367], [377, 383]], [[293, 296], [309, 332], [364, 367], [377, 383]], [[468, 486], [485, 485]], [[501, 501], [572, 572]], [[719, 721], [771, 772]], [[795, 797], [806, 808]], [[806, 811], [1107, 1114], [1122, 1129], [1136, 1136], [1141, 1141]], [[806, 811], [1107, 1114], [1122, 1129], [1136, 1136], [1141, 1141], [1188, 1196], [1333, 1341], [1352, 1360]], [[810, 811], [1093, 1096], [1111, 1114], [1126, 1129]], [[810, 811], [1093, 1096], [1111, 1114], [1126, 1129], [1162, 1166], [1192, 1196], [1337, 1341], [1356, 1360], [1375, 1378], [1380, 1380], [1420, 1424]], [[868, 873], [905, 910], [950, 951], [970, 975], [1002, 1007]], [[871, 873], [897, 899], [908, 910], [973, 975], [1005, 1007], [1107, 1109], [1122, 1124]], [[871, 873], [897, 899], [908, 910], [973, 975], [1005, 1007], [1107, 1109], [1122, 1124], [1175, 1177], [1188, 1190], [1206, 1208], [1333, 1335], [1352, 1354], [1461, 1463], [1489, 1491], [1500, 1502]], [[871, 873], [897, 899], [908, 910], [973, 975], [1005, 1007], [1107, 1109], [1122, 1124], [1175, 1177], [1188, 1190], [1206, 1208], [1333, 1335], [1352, 1354], [1461, 1463], [1489, 1491], [1500, 1502], [1536, 1538], [1565, 1567], [1642, 1644], [1716, 1718], [1735, 1737], [1775, 1777]], [[1099, 1099], [1171, 1171]], [[1198, 1198], [1370, 1370]], [[1348, 1360], [1391, 1393]], [[1349, 1349], [1392, 1392]], [[1458, 1463], [1497, 1502]], [[1458, 1463], [1497, 1502], [1533, 1538], [1562, 1567], [1617, 1618]], [[1557, 1560], [1706, 1707]], [[1624, 1632], [1716, 1724], [1735, 1743]], [[1628, 1632], [1720, 1724], [1739, 1743], [1779, 1783]], [[1630, 1630], [1722, 1722]], [[1929, 1929], [1939, 1939]], [[1929, 1930], [1939, 1940]], [[2099, 2101], [2128, 2130]], [[2099, 2104], [2128, 2133]], [[2103, 2104], [2132, 2133]], [[2220, 2222], [2229, 2229], [2318, 2320]], [[2328, 2330], [2338, 2338]], [[2415, 2420], [2502, 2503]], [[2872, 2874], [2925, 2927]], [[2876, 2880], [2947, 2951]], [[3122, 3124], [3130, 3136], [3196, 3197]], [[3268, 3272], [3417, 3423]], [[3803, 3808], [3803, 3809], [3834, 3836]], [[3803, 3808], [3834, 3836]], [[3892, 3894], [4033, 4035], [4061, 4063], [4152, 4154], [4181, 4183], [4315, 4317]], [[3913, 3917], [4037, 4041], [4200, 4204], [4319, 4323]], [[3913, 3917], [4037, 4041], [4200, 4204], [4319, 4323], [4432, 4436]], [[3999, 3999], [4242, 4242]], [[4033, 4041], [4315, 4323]], [[4033, 4041], [4315, 4323], [4428, 4436]], [[4045, 4047], [4164, 4166], [4327, 4329]], [[4202, 4202], [4321, 4321]], [[4344, 4346], [4428, 4430]]]\n"
     ]
    }
   ],
   "source": [
    "print(doc._.coref_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598f5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANI, average nucleotide identity; dDDH, digital DNA–DNA hybridization; KEGG, Kyoto Encyclopedia of Genes and Genomes; R2A, Reasoner's 2A. The newly sequenced data included in this work are deposited under the nucleotide accession numbers: MZ919349 and MZ920050 and under the Bioproject accession numbers JAIQDI000000000, JAJNEC000000000 and JAIQDJ000000000 at a public domain server in the National Center for Biotechnology Information database. All supporting data, code and protocols have been provided within the article or through supplementary data files. A supplementary table and further supplementary files can be found at: https://doi.org/10.6084/m9.figshare.18220907.v1. Although members of the genera Niabella and Thermomonas are often isolated from similar environmental samples, they belong to the Bacteroidetes and Proteobacteria, respectively. The genus Niabella is a member of the family Chitinophagaceae. Cells of Niabella species are Gram-stain-negative, aerobic, non-flagellated and short rods. Since the genus Niabella was firstly described by Kim et al., 11 specific names have been validly published. The names of ‘Niabella terrae’ and ‘Niabella thaonhiensis’ were proposed but have not been validated at the time of this writing. The genus Thermomonas, a member of the family Xanthomonadaceae, was firstly described by Busse et al.. At the time of writing, seven species have been validly published. The species of the genera Niabella and Thermomonas have been isolated from a wide range of habitats, and they share features such as cells that are Gram-strain-negative, aerobic and non-flagellated. In this report, we describe two bacterial strains that were isolated from a constructed wetland system, by phylogenetic, physiological, biochemical, and genomic analyses. The constructed wetland system, also called the Dragon-shaped Wetland, located in the central area of the Beijing Olympic Park area, is the largest urban artificial water system in Asia. The water in the Dragon-shaped Water System comes mainly from the Beijing Qinghe Water Reclamation Plant, and the entire water system has a complete circulation system. Sludge samples from the Dragon-shaped Wetland water system were collected, and were serially diluted with 0.85 % NaCl and plated onto Reasoner's 2A agar. The isolates, designated as 3A5MI-3T and RSS-23T, were obtained after incubation for 3 days at 30 °C and were routinely stored at −80 °C as suspensions in R2A broth supplemented with 20 % glycerol. Genomic DNA was extracted with a commercial TIANamp Bacteria DNA Kit, and 16S rRNA genes were amplified by PCR with universal bacterial primers 27 F and 1492R, which was also used for sequencing the PCR product. The almost-complete sequence was compared with 16S rRNA gene sequences from GenBank. The 16S rRNA gene sequences were aligned using ClustalW. Phylogenetic analyses were carried out using three phylogenetic algorithms: neighbour-joining, maximum-likelihood and maximum-parsimony. Phylogenetic trees were reconstructed and bootstrapped with 1000 replicates of each sequence using mega version 7.0. The CVTree method was used to reconstruct phylogenomic tree based on whole genomes. Analysis of 16S rRNA gene sequences in GenBank indicated that strain 3A5MI-3T was phylogenetically close to Niabella pedocola R384T, ‘Niabella thaonhiensis’ NHI-24T, Niabella aurantiaca R2A15-11T, Niabella tibetensis 15-4T, Niabella soli JS 13-8T, Niabella drilacis E90T, Niabella hirudinis CCM8411T, Niabella aquatica RP-2T, Niabella yanshanensis CCBAU 05354T, Niabella ginsenosidivorans BS26T, ‘Niabella terrae’ ICM 1-15T, Niabella hibiscisoli THG-DN5.5T and Niabella ginsengisoli GR 10-1T, as well as to Terrimonas rhizosphaerae CR94T and was &lt;92 % similar to some other species included in this analysis. Based on the results of phylogenetic analysis and 16S rRNA gene identity, N. pedocola JCM 31011T was selected as a reference strain for phenotypic tests. Strain RSS-23T was phylogenetically close to Thermomonas aquatica SY21T, Thermomonas fusca LMG 21737T, Thermomonas brevis LMG 21746T, Thermomonas carbonis GZ436T, Thermomonas haemolytica A50-7-3T, Thermomonas koreensis NBRC 101155T and Thermomonas hydrothermalis SGM-6T. The phylogenetic tree showed that the strain grouped with seven members of the genus Thermomonas. A phylogenomic tree based on genome sequences also showed the phylogenetic positions of strains 3A5MI-3T and RSS-23T. To analyse the genomic properties of strains 3A5MI-3T and RSS-23T, whole-genome sequencing was performed using the Illumina system. The genomic assembly was performed with the SPAdes software using clean data. Average nucleotide identity values were calculated with ChunLab’s online ANI Calculator. Digital DNA–DNA hybridization was performed by using the Genome-to-Genome Distance Calculator. The genome of strain 3A5MI-3T contained three contigs, and the total length was 6.59 Mb, encoding 6358 genes. The calculated DNA G+C content of strain 3A5MI-3T was 47.07 mol%. The genome of strain 3A5MI-3T carries 49 ncRNA genes, including three rRNA operons, 41 tRNA genes and five sRNA genes. In addition, there is a clustered regularly interspaced short palindromic repeat sequence with a length of 2862 bp in this genome. The Kyoto Encyclopedia of Genes and Genomes functional category distribution revealed that large numbers of genes in the genome of strain 3A5MI-3T belonged to the metabolism, genetic information processing, environmental information processing and cellular processes. Annotated with the RefSeq non-redundant proteins database, the genomes of strain 3A5MI-3T harboured nine genes encoding phosphohydrolase, nine genes encoding phosphatase, 11 genes encoding glucosidase, 51 genes encoding glycosyl hydrolase families, 36 genes encoding starch binding protein, six genes encoding polysaccharide lyase, seven genes encoding polysaccharide deacetylase, 28 genes encoding SusC/RagA family TonB-linked outer membrane protein, 13 genes encoding xylanase, one gene encoding cellulase and one gene encoding chondroitinase-B. We found genes for xylanase and cellulase in the strain 3A5MI-3T and N. pedocola JCM31011T genomes. APIZYM and carbon source utilization experiments confirmed that strain 3A5MI-3T and N. pedocola JCM31011T could utilize xylan and cellulose. Although strain 3A5MI-3T and N. pedocola JCM31011T both have the chitinase gene in their genome, neither of them can use chitin. Trypsin-like serine proteases were found in the strain 3A5MI-3T genome but not in strain N. pedocola JCM31011T, which corresponds to APIZYM trypsin activity for strain 3A5MI-3T but not for N. pedocola JCM31011T. Comparing the genomes of strain 3A5MI-3T and N. pedocola JCM 31011T, we found that the specific genes detected in strain 3A5MI-3T were as follows: four genes encoding lipopolysaccharide biosynthesis protein, three genes encoding biotin synthase, three genes encoding CRISPR-associated protein Cas1, two genes encoding 2-dehydropantoate 2-reductase, one gene encoding bifunctional NAD(P)H-nitrite reductase/anaerobic dehydrogenase, two genes encoding arabinogalactan endo-1,4-beta-galactosidase, three genes encoding TDP-4-oxo-6-deoxy-d-glucose aminotransferase, one gene encoding zinc-binding alcohol dehydrogenase family protein, three genes encoding 3-phytase, three genes encoding alpha-glucuronidase and one gene encoding Alg9-like mannosyltransferase family protein. The accession numbers of the sequences in the database relating to the specific or characteristic genes of strain 3A5MI-3T and N. pedocola JCM 31011T are provided as supplementary material. The ANI value between strains 3A5MI-3T and N. pedocola JCM 31011T was calculated using ChunLab’s online ANI Calculator. We performed genome sequencing of N. pedocola JCM31011T and its GenBank/EMBL/DDBJ accession number is JAJNEC000000000. The ANI result was 77.48 %, below the 95 % inter-species threshold. dDDH was performed with genome sequences of strains 3A5MI-3T and N. pedocola JCM 31011T using the Genome-to-Genome Distance Calculator 29]. The estimated dDDH value was 19.50 %, which was below the standard value generally recommended for species differentiation. The genome of strain RSS-23T contained also three contigs, and the total length is 2.79 Mb, encoding 2575 genes. The calculated genomic DNA G+C content of strain RSS-23T was 61.21 mol%. The genome of strain RSS-23T carries 58 ncRNA genes, including six rRNA genes, 49 tRNA genes and three sRNA genes. The KEGG functional category distribution revealed that large numbers of genes in the genome of strain RSS-23T belonged to the metabolism, genetic information processing, environmental information processing, cellular processes. Annotated with the RefSeq NR database, the genomes of strain RSS-23T harboured 61 genes encoding phosphatase, six genes encoding glucosidase, four genes encoding glycosyl hydrolase, 27 genes encoding glycosyl transferase and 11 genes encoding polysaccharide biosynthesis protein. APIZYM positive enzyme genes such as alkaline phosphatase, acid phosphatase and α-glucosidase were also found in the genome. Comparing the genomes of strains RSS-23T and T. fusca DSM 15424T, we found that the specific genes contained in strain RSS-23T are as follows: three genes encoding aminotransferase class I/II, six genes encoding chloride channel protein, three genes encoding penicillin acylase, three genes encoding UDP-glucose 4-epimerase, three genes encoding UDP-N-acetylglucosamine 2-epimerase and three genes encoding carbonate dehydratase. The accession numbers of the sequences on the database relating to the specific or characteristic genes of strain RSS-23T and T. fusca DSM 15424T are provided as supplementary files. The ANI values between strain RSS-23T and T. fusca DSM 15424T or T. aquatica SY21T were 78.10 and 76.11 %, respectively, which were below the 95 % inter-species threshold. The estimated dDDH values of strain RSS-23T to T. fusca DSM 15424T and T. aquatica SY21T were 25.20 and 20.90 %, respectively, which were below the standard value generally recommended for species differentiation. The morphology and size of cells grown on R2A agar at 30 °C for 2 days were observed by using transmission electron microscopy, and motility was observed with light microscopy. Gram-staining was performed according to Hucker. Growth was measured at temperatures of 16, 20, 30, 37,40, 45, 50, 60, 65 and 70 °C. Anaerobic growth was examined in R2A broth without oxygen, and using l-cysteine to consume residual oxygen and resazurin as a redox indicator. Salt tolerance was investigated by supplementing NaCl concentrations of 0, 1, 2, 3, 4 and 5.0 % to R2A broth. The effect of pH was tested in R2A broth using phosphate buffer and Tris buffer. Catalase activity was determined using a 3 % hydrogen peroxide solution. Oxidation of N,N,N′,N′-tetramethyl-p-phenylenediamine dihydrochloride was used to check oxidase activity. Monosaccharide, disaccharide and polysaccharide utilization was tested with the following compounds and basic inorganic salt at concentration of 1 g 1−1: starch, cellulose, chitin, xylan, agar, fructose, galactose, mannose, xylose, arabinose, glucose, lactose, maltose, cellobiose, trehalose, sucrose and sorbose. Bacterial cells grown in R2A were collected and washed three times with basic inorganic salt medium, inoculated at 1 %, and cultivated for 2 days. Other physiological and biochemical analyses were carried out by using the API ZYM, API 20NE and Biolog GEN III systems, according to the manufacturers’ instructions. Cells of strains 3A5MI-3T and RSS-23T were Gram-stain-negative, aerobic, non-motile and short rod-shaped. The distinctive physiological characteristics of strains 3A5MI-3T and RSS-23T are listed in Tables 1 and 2, respectively. The cellular fatty acids were determined using cells grown on R2A agar for 2 days at 30 °C with the standard midi protocol and analysed with a gas chromatograph. The extraction, purification and analysis of quinones were carried out according to the methods of Collins et al.. The total lipids were extracted using chloroform and methanol, followed by thin-plate biphasic chromatography to identify each lipid component. Strain 3A5MI-3T contained aliphatic saturated fatty acid as its predominant cellular fatty acid that accounted for about half of total fatty acids, followed by iso-C15 : 1 ω6c and/or iso-C15 : 1 ω7c, iso-C17 : 0 3-OH and summed feature 3. The composition of the fatty acids is shown in Table 3. The major cytoquinone was determined to be MK-7, as previously reported for described members of the genus Niabella. The polar lipid profile of strain 3A5MI-3T contained phosphatidylethanolamine and three unknown lipids. Strain RSS-23T also contained aliphatic saturated fatty acid as its predominant cellular fatty acid, which accounted for about one third of total fatty acids. The fatty acid profiles are shown in Table 4. The major isoprenoid quinone was determined to be Q-8, as previously reported for members of the genus Thermomonas. The polar lipid profile contained diphosphatidylglycerol, phosphatidylglycerol, phosphatidylethanolamine, two unknown phospholipids and an unknown lipid. Based on the phylogenetic and phenotypic characteristics, we conclude that strains 3A5MI-3T and RSS-23T represent novel species of the genera Niabella and Thermomonas, respectively, for which the names Niabella beijingensis sp. nov. and Thermomonas beijingensis sp. nov. are proposed. Niabella beijingensis. Cells are Gram-stain-negative, strictly aerobic, non-motile and short rod-shaped. Colonies are convex, circular, smooth and edges were regular after incubation on R2A broth for 2 days at 30 °C. The strain can grow in a wide range of temperature and pH, and the range of NaCl tolerance is 0–2 % NaCl. Optimal growth at 30 °C, pH 7.0 and with 1 % NaCl. Oxidase and catalase activities are positive and negative, respectively. According to the API 20NE system, nitrate cannot be reduced to nitrite. Positive for aesculin ferric citrate, gelatin, 4-nitrophenyl-β-d-galactopyranoside d-glucose, l-arabinose, d-mannose, N-acetyl-glucosamine and maltose, and negative for l-tryptophan, l-arginine, urea, d-mannitol, potassium gluconate, capric acid, adipic acid, malic acid, trisodium citrate and phenylacetic acid. The following enzyme activities are positive: alkaline phosphatase, leucine arylamidase, valine arylamidase, acid phosphatase, naphthol-AS-BI-phosphohydrolase, α-galactosidase, β-galactosidase, α-glucosidase, β-glucosidase, N-acetyl-β-glucosaminidase, α-mannosidase and α-fucosidase. Esterase lipase, cystine arylamidase and trypsin are weak; and esterase, lipase, α-chymotrypsin and β-glucuronidase are negative. Carbon substrates utilized include gentiobiose, sucrose, stachyose, maltose, trehalose, cellobiose, turanose, raffinose, lactose, melibiose, d-mannose, d-fructose, d-galactose, l-rhamnose, glycy-l-proline, l-aspartic acid, l-glutamic acid, l-serine, dextrin, pectin, Tween 40, acetoacetic acid, acetic acid, l-lactic acid, N-acetyl neuraminic acid, d-galacturonic acid, d-glucuronic acid, l-galactonic acid lactone, methyl β-d-glucoside, d-salicin, N-acetyl-d-glucosamine, N-acetyl-d-galactosamine, glucuronamide, gelatin, d-arabinose, d-sorbose, starch and agar. Weak utilization of l-fucose, d-glucose-6-PO4, l-alanine, l-histidine, d-gluconic acid, citric acid, glycerol, N-acetyl-β-d-mannosamine, methyl pyruvate, xylan and cellulose. No utilization of 3-methyl glucose, d-fucose, inosine, d-sorbitol, d-mannitol, d-arabitol, myo-inositol, d-fructose-6-PO4, d-aspartic acid, d-serine, l-arginine, l-pyroglutamic acid, mucic acid, quinic acid, d-saccharic acid, p-hydroxy-phenylacetic acid, d-lactic acid methyl ester, α-keto-glutaric acid, d-malic acid, l-malic acid, bromo-succinic acid, γ-amino-butryric acid, β-hydroxy-d,l-butyric acid, α-keto-butyric acid, propionic acid, formic acid and chitin. The predominant cellular fatty acids are iso-C15 : 0, iso-C15 : 1 ω6c and/or iso-C15 : 1 ω7c, iso-C17 : 0 3-OH and summed feature 3. Contains cytoquinone MK-7 as respiratory quinone. The polar lipid profile contains phosphatidylethanolamine and three unknown lipids. The G+C content of the type strain is 47.07 mol%. The type strain, 3A5MI-3T, was isolated from constructed wetland sludge from Beijing, PR China. Thermomonas beijingensis. Cells are Gram-stain-negative, aerobic, non-motile and rod-shaped. Colonies are convex, circular, smooth and edges are regular after incubation on R2A broth for 2 days at 30 °C. The strain can grow in a wide range of temperature and pH. The range of NaCl tolerance is 0–1 % NaCl. Optimal growth at 30 °C, pH 7.0 and with 1 % NaCl. Oxidase and catalase activities are positive. According to the API 20NE system, nitrate can be reduced to nitrite. Positive for aesculin ferric citrate, gelatine, d-glucose, N-acetyl-glucosamine and maltose, and negative for l-tryptophan, l-arginine, urea, 4-nitrophenyl-β-d-galactopyranoside, l-arabinose, d-mannose, d-mannitol, potassium gluconate, caprate, adipate, malate, citrate and phenylacetate. The following enzyme activities of alkaline phosphatase, leucine arylamidase, valine arylamidase, acid phosphatase, naphthol-AS-BI-phosphohydrolase, α-glucosidase, β-glucosidase, N-acetyl-β-glucosaminidase, esterase lipase, cystine arylamidase, trypsin, esterase and α-chymotrypsin are positive; α-mannosidase. α-fucosidase, lipase, α-galactosidase, β-galactosidase and β-glucuronidase are negative. Carbon sources utilized include dextrin, maltose, cellobiose, gentiobiose, N-acetyl-d-glucosamine, N-acetyl-d-galactosamine, gelatin, glycy-l-proline, l-alanine, l-arginine, l-aspartic acid, l-glutamic acid, l-serine, methyl pyruvate, Tween 40, α-hydroxy-butyric acid, β-hydroxy-d,l-butyric acid, α-keto-butyric acid, acetoacetic acid, propionic acid, acetic acid, d-arabinose, d-sorbose, starch, xylan and agar. Weak utilization of turanose, N-acetyl-β-d-mannosamine, d-fructose, d-glucose-6-PO4, d-aspartic acid and pectin. No growth occurs on trehalose, sucrose, stachyose, raffinose, lactose, melibiose, methyl β-d-glucoside, d-salicin, N-acetyl neuraminic acid, α-d-glucose, d-mannose, d-galactose, 3-methyl glucose, d-fucose, l-fucose, l-rhamnose, inosine, d-sorbitol, d-mannitol, d-arabitol, myo-inositol, glycerol, d-serine, l-histidine, l-pyroglutamic acid, d-galacturonic acid, l-galactonic acid lactone, d-gluconic acid, d-glucuronic acid, glucuronamide, mucic acid, quinic acid, d-saccharic acid, p-hydroxy-phenylacetic acid, d-lactic acid methyl ester, l-lactic acid, citric acid, α-keto-glutaric acid, d-malic acid, l-malic acid, bromo-succinic acid, γ-amino-butryric acid, formic acid, cellulose and chitin. The predominant cellular fatty acids are iso-C15 : 0, iso-C17 : 1 ω9c, iso-C11 : 0 3-OH and summed feature 3. The major isoprenoid quinone is Q-8. The polar lipid profile contains diphosphatidylglycerol, phosphatidylglycerol, phosphatidylethanolamine, two unknown phospholipids and an unknown lipid. The type strain, RSS-23T, was isolated from soil of the Dragon-shaped Wetland System in Beijing Olympic Park, PR China. The DNA G+C content of the type strain is 61.21 mol%. This work was financially supported by National Natural Science Foundation of China. We thank Prof. Yuguang Zhou at Institute of Microbiology, Chinese Academy of Sciences for coordination of deposits of type strains. The authors declare that there are no conflicts of interest. Phenotypic characteristics that differentiate strain 3A5MI-3T from the phylogenetically closely related strains of the genus NiabellaStrains: 1, 3A5MI-3T; 2, N. pedocola JCM 31011T; 3, N. aquatica JCM 30952T; 4, N. aurantiaca DSM 17617T; 5, N. ginsengisoli JCM 15444T; 6, N. ginsenosidivorans JCM 18199T; 7, N. hibiscisoli KACC 18857T; 8, N. hirudinis DSM 25812T; 9, N. soli DSM 19437T; 10, ‘N. terrae’ JCM 19502T; 11, ‘N. thaonhiensis’ JCM 18864T; 12, N. yanshanensis KACC 14980T; 13, N. drilacis DSM25812T; 14, N. tibetensis CCTCC AB 209167T. +, Positive; −, negative; w, weakly positive. The data of strain 3A5MI-3T and N. pedocola JCM 31011T were taken from the current study, others were taken from the published literature. Phenotypic characteristics that differentiate strain RSS-23T from the phylogenetically closely related species of the genus ThermomonasStrains: 1, RSS-23T; 2, T. fusca DSM 15424T; 3, T. aquatica KCTC 62191T; 4, T. carbonis KCTC 42013T; 5, T. brevis DSM 15422T; 6, T. haemolytica DSM 13605T; 7, T. koreensis KCTC 12540T; 8, T. hydrothermalis DSM 14834T. +, Positive; −, negative; w, weakly positive/sensitive. The data of strain RSS-23T and T. fusca DSM 15424T were taken from the current study, others were taken from the published literature. Cellular fatty acid composition of strain 3A5MI-3T and closely related species of the genus NiabellaStrains: 1, 3A5MI-3T; 2, N. pedocola JCM 31011T; 3, N. aquatica JCM 30952T; 4, N. aurantiaca DSM 17617T; 5, N. ginsengisoli JCM 15444T; 6, N. ginsenosidivorans JCM 18199T; 7, N. hibiscisoli KACC 18857T; 8, N. hirudinis DSM 25812T; 9, N. soli DSM 19437T; 10, ‘N. terrae’ JCM 19502T; 11, ‘N. thaonhiensis’ JCM 18864T; 12, N. yanshanensis KACC 14980T ; 13，N. drilacis DSM25812T; 14, N. tibetensis CCTCC AB 209167T. −, Not detected or &lt;0.1 %. The data of strain 3A5MI-3T and N. pedocola JCM 31011T were taken from the current study, others were taken from the published literature. Cellular fatty acid composition of strain RSS-23T and closely related species of the genus ThermomonasStrains: 1, RSS-23T; 2, T. fusca DSM 15424T; 3, T. aquatica KCTC 62191T; 4, T. carbonis KCTC 42013T; 5, T. brevis DSM 15422T; 6, T. haemolytica DSM 13605T; 7, T. koreensis KCTC 12540T; 8, T. hydrothermalis DSM 14834T. −, Not detected or &lt;0.1 %. The data of strain RSS-23T and T. fusca DSM 15424T were taken from the current study, others were taken from the published literature."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8b061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1\n",
      "this work\n",
      "the article\n",
      "this report\n",
      "\n",
      "Cluster 2\n",
      "Bioproject\n",
      "Reasoner's\n",
      "\n",
      "Cluster 3\n",
      "members of the genera Niabella and Thermomonas\n",
      "they\n",
      "\n",
      "Cluster 4\n",
      "the genera Niabella\n",
      "The genus Niabella\n",
      "the genus Niabella\n",
      "\n",
      "Cluster 5\n",
      "the genera Niabella and Thermomonas\n",
      "the genera Niabella and Thermomonas\n",
      "\n",
      "Cluster 6\n",
      "Thermomonas\n",
      "Thermomonas\n",
      "\n",
      "Cluster 7\n",
      "seven species\n",
      "The species of the genera Niabella and Thermomonas\n",
      "they\n",
      "\n",
      "Cluster 8\n",
      "a constructed wetland system\n",
      "The constructed wetland system, also called the Dragon-shaped Wetland, located in the central area of the Beijing Olympic Park area\n",
      "the Dragon-shaped Water System\n",
      "the entire water system\n",
      "the Dragon-shaped Wetland water system\n",
      "\n",
      "Cluster 9\n",
      "a constructed wetland system\n",
      "The constructed wetland system, also called the Dragon-shaped Wetland, located in the central area of the Beijing Olympic Park area\n",
      "the entire water system\n",
      "the Dragon-shaped Wetland water system\n",
      "\n",
      "Cluster 10\n",
      "PCR with universal bacterial primers 27 F and 1492R, which was also used for sequencing the PCR product\n",
      "PCR\n",
      "\n",
      "Cluster 11\n",
      "GenBank\n",
      "GenBank\n",
      "\n",
      "Cluster 12\n",
      "Strain RSS-23T\n",
      "the strain\n",
      "\n",
      "Cluster 13\n",
      "strains 3A5MI-3T\n",
      "strains 3A5MI-3T\n",
      "\n",
      "Cluster 14\n",
      "strains 3A5MI-3T and RSS-23T\n",
      "strain 3A5MI-3T and N. pedocola JCM31011T\n",
      "strain 3A5MI-3T and N. pedocola JCM31011T\n",
      "their\n",
      "them\n",
      "\n",
      "Cluster 15\n",
      "strains 3A5MI-3T and RSS-23T\n",
      "strain 3A5MI-3T and N. pedocola JCM31011T\n",
      "strain 3A5MI-3T and N. pedocola JCM31011T\n",
      "their\n",
      "them\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "strains 3A5MI-3T and N. pedocola JCM 31011T\n",
      "\n",
      "Cluster 16\n",
      "RSS-23T\n",
      "N. pedocola JCM31011T\n",
      "N. pedocola JCM31011T\n",
      "N. pedocola JCM31011T\n",
      "\n",
      "Cluster 17\n",
      "RSS-23T\n",
      "N. pedocola JCM31011T\n",
      "N. pedocola JCM31011T\n",
      "N. pedocola JCM31011T\n",
      "strain N. pedocola JCM31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM31011T\n",
      "its\n",
      "N. pedocola JCM 31011T\n",
      "\n",
      "Cluster 18\n",
      "The genome of strain 3A5MI-3T\n",
      "The genome of strain 3A5MI-3T\n",
      "this genome\n",
      "the genome of strain 3A5MI-3T\n",
      "the genomes of strain 3A5MI-3T\n",
      "\n",
      "Cluster 19\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "\n",
      "Cluster 20\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strains 3A5MI-3T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "\n",
      "Cluster 21\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strains 3A5MI-3T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "\n",
      "Cluster 22\n",
      "APIZYM\n",
      "APIZYM\n",
      "\n",
      "Cluster 23\n",
      "we\n",
      "We\n",
      "\n",
      "Cluster 24\n",
      "The ANI value between strains 3A5MI-3T and N. pedocola JCM 31011T\n",
      "The ANI result\n",
      "\n",
      "Cluster 25\n",
      "ANI\n",
      "ANI\n",
      "\n",
      "Cluster 26\n",
      "The genome of strain RSS-23T\n",
      "The genome of strain RSS-23T\n",
      "\n",
      "Cluster 27\n",
      "The genome of strain RSS-23T\n",
      "The genome of strain RSS-23T\n",
      "the genome of strain RSS-23T\n",
      "the genomes of strain RSS-23T\n",
      "the genome\n",
      "\n",
      "Cluster 28\n",
      "the RefSeq NR database\n",
      "the database\n",
      "\n",
      "Cluster 29\n",
      "strains RSS-23T and T. fusca DSM 15424T\n",
      "strain RSS-23T and T. fusca DSM 15424T\n",
      "strain RSS-23T and T. fusca DSM 15424T\n",
      "\n",
      "Cluster 30\n",
      "T. fusca DSM 15424T\n",
      "T. fusca DSM 15424T\n",
      "T. fusca DSM 15424T\n",
      "T. fusca DSM 15424T\n",
      "\n",
      "Cluster 31\n",
      "DSM\n",
      "DSM\n",
      "\n",
      "Cluster 32\n",
      "R2A\n",
      "R2A\n",
      "\n",
      "Cluster 33\n",
      "R2A broth\n",
      "R2A broth\n",
      "\n",
      "Cluster 34\n",
      "strains 3A5MI-3T\n",
      "strains 3A5MI-3T\n",
      "\n",
      "Cluster 35\n",
      "strains 3A5MI-3T and RSS-23T\n",
      "strains 3A5MI-3T and RSS-23T\n",
      "\n",
      "Cluster 36\n",
      "RSS-23T\n",
      "RSS-23T\n",
      "\n",
      "Cluster 37\n",
      "Strain 3A5MI-3T\n",
      "its\n",
      "strain 3A5MI-3T\n",
      "\n",
      "Cluster 38\n",
      "Strain RSS-23T\n",
      "its\n",
      "\n",
      "Cluster 39\n",
      "strains 3A5MI-3T and RSS-23T\n",
      "The strain\n",
      "\n",
      "Cluster 40\n",
      "l-fucose\n",
      "d-fucose\n",
      "\n",
      "Cluster 41\n",
      "d-glucose-6-PO4\n",
      "d-fructose-6-PO4\n",
      "\n",
      "Cluster 42\n",
      "the type strain\n",
      "The type strain, 3A5MI-3T,\n",
      "The strain\n",
      "\n",
      "Cluster 43\n",
      "N-acetyl-glucosamine\n",
      "N-acetyl-d-glucosamine\n",
      "\n",
      "Cluster 44\n",
      "The type strain, RSS-23T\n",
      "The type strain, RSS-23T,\n",
      "the type strain\n",
      "\n",
      "Cluster 45\n",
      "The type strain, RSS-23T\n",
      "the type strain\n",
      "\n",
      "Cluster 46\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "strain 3A5MI-3T\n",
      "strain 3A5MI-3T\n",
      "\n",
      "Cluster 47\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "\n",
      "Cluster 48\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "N. pedocola JCM 31011T\n",
      "T. fusca DSM 15424T\n",
      "\n",
      "Cluster 49\n",
      "KACC\n",
      "KACC\n",
      "\n",
      "Cluster 50\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "\n",
      "Cluster 51\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "strain 3A5MI-3T and N. pedocola JCM 31011T\n",
      "strain RSS-23T and T. fusca DSM 15424T\n",
      "\n",
      "Cluster 52\n",
      "the current study\n",
      "the current study\n",
      "the current study\n",
      "\n",
      "Cluster 53\n",
      "JCM\n",
      "JCM\n",
      "\n",
      "Cluster 54\n",
      "strain RSS-23T\n",
      "strain RSS-23T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for item in doc._.coref_clusters:\n",
    "    print (f'Cluster {i}')\n",
    "    for span in item:\n",
    "        start, end = span\n",
    "        print(doc[start:end+1])\n",
    "    i = i+1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c462f3",
   "metadata": {},
   "source": [
    "# Result:\n",
    "crosslingual_coreference is not a good solution when it comes to scientific articles.\n",
    "The model has to be trained on the specific domain and is weak with longer sentences/texts. Proofed above. Acc about 0,8.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
